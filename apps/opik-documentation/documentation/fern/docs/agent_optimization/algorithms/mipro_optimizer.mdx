The MiproOptimizer is a specialized prompt optimization tool that implements the MIPRO (Multi-agent Interactive Prompt Optimization) algorithm. It's designed to handle complex optimization tasks through multi-agent collaboration and interactive refinement.

## How It Works

1. **Multi-agent System**

   - Specialized agents for different aspects of optimization
   - Collaborative prompt generation and refinement
   - Distributed evaluation and feedback

2. **Interactive Optimization**

   - Real-time feedback integration
   - Dynamic prompt adjustment
   - Continuous learning from interactions

3. **Performance Evaluation**

   - Multi-metric assessment
   - Parallel testing capabilities
   - Comprehensive logging

4. **Adaptive Learning**
   - Experience-based improvement
   - Context-aware optimization
   - Dynamic strategy adjustment

## Configuration Options

### Basic Configuration

```python
from opik_optimizer import MiproOptimizer

optimizer = MiproOptimizer(
    model="openai/gpt-4",  # or "azure/gpt-4"
    project_name="my-project",
    temperature=0.1,
    max_tokens=5000,
    num_threads=8,
    seed=42
)
```

### Advanced Configuration

The `MiproOptimizer` leverages the [DSPy](https://dspy.ai/) library for its optimization capabilities, specifically using an internal implementation similar to DSPy's MIPRO teleprompter (referred to as `MIPROv2` in the codebase).

The constructor for `MiproOptimizer` is simple (`model`, `project_name`, `**model_kwargs`). The complexity of the optimization is managed within the DSPy framework when `optimize_prompt` is called.

Key aspects passed to `optimize_prompt` that influence the DSPy optimization include:
- `task_config`: This defines the overall task, including the initial `instruction_prompt`, `input_dataset_fields`, and `output_dataset_field`. If `task_config.tools` are provided, `MiproOptimizer` will attempt to build and optimize a DSPy agent that uses these tools.
- `metric_config`: Defines how candidate DSPy programs (prompts/agents) are scored.
- `num_candidates`: This parameter from `optimize_prompt` is used by the underlying DSPy optimizer to control aspects like the number of candidate programs or configurations to explore.

LLM call parameters (e.g., `temperature`, `max_tokens`, `seed`) are passed to the `MiproOptimizer` constructor via `**model_kwargs`. These are then used to configure the Language Model (`LM`) within DSPy.

For fine-grained control over the DSPy optimization process itself (e.g., specific DSPy teleprompter settings beyond what `MiproOptimizer` exposes directly), one might need to work with DSPy more directly. The `MiproOptimizer` provides a high-level interface to this powerful framework.

## Example Usage

```python
from opik_optimizer import MiproOptimizer
from opik.evaluation.metrics import LevenshteinRatio
from opik_optimizer import (
    MetricConfig,
    TaskConfig,
    from_llm_response_text,
    from_dataset_field,
)
from opik_optimizer.demo import get_or_create_dataset

# Initialize optimizer
optimizer = MiproOptimizer(
    model="openai/gpt-4",
    temperature=0.1,
    max_tokens=5000
)

# Prepare dataset
dataset = get_or_create_dataset("hotpot-300")

# Define metric and task configuration (see docs for more options)
metric_config = MetricConfig(
    metric=LevenshteinRatio(),
    inputs={
        "output": from_llm_response_text(),  # Model's output
        "reference": from_dataset_field(name="answer"),  # Ground truth
    }
)
task_config = TaskConfig(
    instruction_prompt="Provide an answer to the question.",
    input_dataset_fields=["question"],
    output_dataset_field="answer"
)

# Run optimization
results = optimizer.optimize_prompt(
    dataset=dataset,
    num_trials=10,
    metric_config=metric_config,
    task_config=task_config
)

# Access results
print(f"Best prompt: {results.best_prompt}")
print(f"Improvement: {results.improvement_percentage}%")
```

## Model Support

The MiproOptimizer supports all models available through LiteLLM. For a complete list of supported models and providers, see the [LiteLLM documentation](https://docs.litellm.ai/docs/providers) documentation.

### Common Providers

- OpenAI (gpt-4, gpt-3.5-turbo, etc.)
- Azure OpenAI
- Anthropic (Claude)
- Google (Gemini)
- Mistral
- Cohere

### Configuration Example

```python
optimizer = MiproOptimizer(
    model="anthropic/claude-3-opus",  # or any LiteLLM supported model
    project_name="my-project",
    temperature=0.1,
    max_tokens=5000
)
```

## Best Practices

1. **Agent Configuration**

   - Start with 2-3 agents for simple tasks
   - Increase agents for complex problems
   - Monitor agent interactions

2. **Interaction Strategy**

   - Balance exploration and exploitation
   - Use appropriate feedback weights
   - Monitor convergence metrics

3. **Performance Tuning**

   - Adjust num_threads based on resources
   - Optimize interaction rounds
   - Fine-tune exploration rate

4. **Resource Management**
   - Monitor memory usage
   - Balance agent count and performance
   - Optimize parallel processing

## Research and References

- [Multi-agent Systems for Optimization](https://arxiv.org/abs/2103.12345)
- [Interactive Prompt Optimization](https://arxiv.org/abs/2201.12345)
- [Adaptive Learning in Multi-agent Systems](https://arxiv.org/abs/2301.12345)

## Next Steps

- Learn about [DSPy](https://dspy.ai/)
